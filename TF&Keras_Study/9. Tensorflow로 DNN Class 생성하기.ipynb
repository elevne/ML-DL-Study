{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b54769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        # 초기화처리\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [], 'loss': []\n",
    "        }\n",
    "    \n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def inference(self, x, keep_prob):\n",
    "        # 모델 정의\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "            \n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "            \n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "        \n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "        \n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "    \n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(tf.clip_by_value(y, 1e-10, 1.0)), reduction_indices=[1]))\n",
    "        return cross_entropy\n",
    "    \n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "    \n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        # 학습 처리\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # evaluate() 용으로 작성해두기\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_porb = keep_prob\n",
    "        \n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # evaluate() 용\n",
    "        self._sess = sess\n",
    "        \n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_, Y_ = shuffle(X_train, y_train)\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start = i*batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x:X_train, t:y_train, keep_prob:p_keep\n",
    "                })\n",
    "                \n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x:X_train, t:y_train, keep_prob:1.0\n",
    "            })\n",
    "            \n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x:X_train, t:y_train, keep_prob:1.0\n",
    "            })\n",
    "\n",
    "            self._history['loss'].append('loss_')\n",
    "            self._history['accuracy'].append('accuracy_')\n",
    "            \n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                     'loss:', loss_,\n",
    "                     'accuracy:', accuracy_)\n",
    "                \n",
    "        return self._history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        # 평가처리\n",
    "        return self.accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x:X_test, self._t:y_test, self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82705327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "y_train, y_test = np.eye(10)[y_train.astype(int)], np.eye(10)[y_test.astype(int)]\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_train.shape\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "X_test.shape\n",
    "X_train, X_test = X_train.astype(np.float64), X_test.astype(np.float64)\n",
    "n_in = len(X_train[0])\n",
    "n_in\n",
    "n_hidden = 200\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a143eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(n_in=784, n_hiddens=[200, 200, 200], n_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc50e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.40084717 accuracy: 0.88585\n",
      "epoch: 1 loss: 0.22031508 accuracy: 0.9361\n",
      "epoch: 2 loss: 0.15581676 accuracy: 0.9547667\n",
      "epoch: 3 loss: 0.12221841 accuracy: 0.96465\n",
      "epoch: 4 loss: 0.10068779 accuracy: 0.97108334\n",
      "epoch: 5 loss: 0.08509092 accuracy: 0.9761\n",
      "epoch: 6 loss: 0.073139 accuracy: 0.97941667\n",
      "epoch: 7 loss: 0.06360354 accuracy: 0.9821333\n",
      "epoch: 8 loss: 0.0559769 accuracy: 0.9844\n",
      "epoch: 9 loss: 0.049479917 accuracy: 0.9860333\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용해보기\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=200, p_keep=0.5)\n",
    "    \n",
    "# epoch 3부터 학습이 아예 안되는 것 확인 가능 => 경사소실 문제\n",
    "# class 내에서 loss function 에 clip_by_value로 수정하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214c88f",
   "metadata": {},
   "source": [
    "### 이번에도 Keras로도 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad0d09e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: 8.0375 - acc: 0.1424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Main\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 33us/sample - loss: 7.9974 - acc: 0.1429 - val_loss: 1.7773 - val_acc: 0.3534\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 2.0056 - acc: 0.2502 - val_loss: 1.6467 - val_acc: 0.4225\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.8944 - acc: 0.3023 - val_loss: 1.5432 - val_acc: 0.4538\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.8259 - acc: 0.3243 - val_loss: 1.4488 - val_acc: 0.4883\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.7725 - acc: 0.3428 - val_loss: 1.4118 - val_acc: 0.5185\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.7215 - acc: 0.3603 - val_loss: 1.3152 - val_acc: 0.5474\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 1.7030 - acc: 0.3674 - val_loss: 1.3641 - val_acc: 0.5304\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.6627 - acc: 0.3835 - val_loss: 1.2821 - val_acc: 0.5514\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.6076 - acc: 0.4047 - val_loss: 1.2109 - val_acc: 0.6039\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.5659 - acc: 0.4176 - val_loss: 1.1549 - val_acc: 0.6017\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.5477 - acc: 0.4206 - val_loss: 1.1088 - val_acc: 0.6263\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.5203 - acc: 0.4304 - val_loss: 1.0806 - val_acc: 0.6220\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.4808 - acc: 0.4492 - val_loss: 1.0669 - val_acc: 0.6233\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.4521 - acc: 0.4623 - val_loss: 1.0197 - val_acc: 0.6580\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.4158 - acc: 0.4739 - val_loss: 1.0041 - val_acc: 0.6526\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3938 - acc: 0.4809 - val_loss: 0.9488 - val_acc: 0.6856\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3738 - acc: 0.4867 - val_loss: 0.9355 - val_acc: 0.6975\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.3488 - acc: 0.4900 - val_loss: 0.8851 - val_acc: 0.6989\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3231 - acc: 0.5008 - val_loss: 0.8807 - val_acc: 0.7011\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3237 - acc: 0.4986 - val_loss: 0.8911 - val_acc: 0.6997\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3093 - acc: 0.5033 - val_loss: 0.8869 - val_acc: 0.7006\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3042 - acc: 0.5059 - val_loss: 0.8649 - val_acc: 0.7115\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2943 - acc: 0.5094 - val_loss: 0.8961 - val_acc: 0.7047\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.2919 - acc: 0.5089 - val_loss: 0.8538 - val_acc: 0.7180\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.2850 - acc: 0.5095 - val_loss: 0.8532 - val_acc: 0.7218\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.3059 - acc: 0.5102 - val_loss: 0.8632 - val_acc: 0.7180\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2697 - acc: 0.5196 - val_loss: 0.8572 - val_acc: 0.7198\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2619 - acc: 0.5224 - val_loss: 0.8588 - val_acc: 0.7174\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2481 - acc: 0.5254 - val_loss: 0.8697 - val_acc: 0.7167\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2318 - acc: 0.5275 - val_loss: 0.8787 - val_acc: 0.6822\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.2095 - acc: 0.5374 - val_loss: 0.8699 - val_acc: 0.7106\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1964 - acc: 0.5433 - val_loss: 0.8484 - val_acc: 0.7108\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1822 - acc: 0.5474 - val_loss: 0.8313 - val_acc: 0.7167\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1723 - acc: 0.5547 - val_loss: 0.8131 - val_acc: 0.7179\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.1718 - acc: 0.5532 - val_loss: 0.8369 - val_acc: 0.7177\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1538 - acc: 0.5608 - val_loss: 0.8197 - val_acc: 0.7180\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.1449 - acc: 0.5635 - val_loss: 0.7926 - val_acc: 0.7256\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 1.1281 - acc: 0.5689 - val_loss: 0.8022 - val_acc: 0.7161\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1166 - acc: 0.5721 - val_loss: 0.7871 - val_acc: 0.7240\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.1059 - acc: 0.5793 - val_loss: 0.7790 - val_acc: 0.7223\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0890 - acc: 0.5866 - val_loss: 0.7671 - val_acc: 0.7328\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0858 - acc: 0.5854 - val_loss: 0.7671 - val_acc: 0.7343\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0789 - acc: 0.5913 - val_loss: 0.7548 - val_acc: 0.7301\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0701 - acc: 0.5956 - val_loss: 0.7638 - val_acc: 0.7333\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0616 - acc: 0.5962 - val_loss: 0.7611 - val_acc: 0.7370\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0551 - acc: 0.6020 - val_loss: 0.7561 - val_acc: 0.7332\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0417 - acc: 0.6047 - val_loss: 0.7198 - val_acc: 0.7358\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0461 - acc: 0.6041 - val_loss: 0.7475 - val_acc: 0.7379\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0337 - acc: 0.6061 - val_loss: 0.7303 - val_acc: 0.7459\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 1.0260 - acc: 0.6115 - val_loss: 0.7345 - val_acc: 0.7448\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "n_in = 784\n",
    "n_hiddens = [200, 200, 200]\n",
    "n_out = 10\n",
    "p_keep = 0.5\n",
    "activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in]+n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "# 학습이 잘 안됨... => 웨이트 초기화 기법을 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83d93162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 1.3713 - acc: 0.5203 - val_loss: 0.4297 - val_acc: 0.8681\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.4661 - acc: 0.8617 - val_loss: 0.2228 - val_acc: 0.9345\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.3364 - acc: 0.9033 - val_loss: 0.1791 - val_acc: 0.9475\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.2760 - acc: 0.9212 - val_loss: 0.1469 - val_acc: 0.9555\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2394 - acc: 0.9314 - val_loss: 0.1347 - val_acc: 0.9597\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2175 - acc: 0.9375 - val_loss: 0.1226 - val_acc: 0.9636\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.2016 - acc: 0.9432 - val_loss: 0.1103 - val_acc: 0.9669\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1885 - acc: 0.9470 - val_loss: 0.1041 - val_acc: 0.9695\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1746 - acc: 0.9506 - val_loss: 0.0986 - val_acc: 0.9717\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1687 - acc: 0.9524 - val_loss: 0.1024 - val_acc: 0.9706\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1591 - acc: 0.9550 - val_loss: 0.1024 - val_acc: 0.9702\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.1547 - acc: 0.9558 - val_loss: 0.0957 - val_acc: 0.9734\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1466 - acc: 0.9582 - val_loss: 0.0877 - val_acc: 0.9761\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1423 - acc: 0.9595 - val_loss: 0.0870 - val_acc: 0.9745\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1365 - acc: 0.9605 - val_loss: 0.0861 - val_acc: 0.9758\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1345 - acc: 0.9618 - val_loss: 0.0875 - val_acc: 0.9763\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1310 - acc: 0.9630 - val_loss: 0.0826 - val_acc: 0.9772\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1265 - acc: 0.9646 - val_loss: 0.0843 - val_acc: 0.9774\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1231 - acc: 0.9646 - val_loss: 0.0844 - val_acc: 0.9756\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1208 - acc: 0.9650 - val_loss: 0.0862 - val_acc: 0.9755\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1162 - acc: 0.9664 - val_loss: 0.0796 - val_acc: 0.9783\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1144 - acc: 0.9671 - val_loss: 0.0762 - val_acc: 0.9782\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1144 - acc: 0.9667 - val_loss: 0.0795 - val_acc: 0.9771\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1088 - acc: 0.9689 - val_loss: 0.0768 - val_acc: 0.9780\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1071 - acc: 0.9690 - val_loss: 0.0809 - val_acc: 0.9776\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1052 - acc: 0.9690 - val_loss: 0.0752 - val_acc: 0.9797\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1029 - acc: 0.9697 - val_loss: 0.0797 - val_acc: 0.9792\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1027 - acc: 0.9701 - val_loss: 0.0750 - val_acc: 0.9792\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.1013 - acc: 0.9699 - val_loss: 0.0698 - val_acc: 0.9805\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0974 - acc: 0.9716 - val_loss: 0.0761 - val_acc: 0.9790\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0973 - acc: 0.9719 - val_loss: 0.0768 - val_acc: 0.9794\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0978 - acc: 0.9718 - val_loss: 0.0786 - val_acc: 0.9782\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0923 - acc: 0.9729 - val_loss: 0.0750 - val_acc: 0.9800\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0949 - acc: 0.9719 - val_loss: 0.0770 - val_acc: 0.9796\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0939 - acc: 0.9721 - val_loss: 0.0734 - val_acc: 0.9799\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0912 - acc: 0.9735 - val_loss: 0.0722 - val_acc: 0.9804\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0911 - acc: 0.9739 - val_loss: 0.0742 - val_acc: 0.9794\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0916 - acc: 0.9737 - val_loss: 0.0715 - val_acc: 0.9799\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0880 - acc: 0.9731 - val_loss: 0.0699 - val_acc: 0.9807\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0858 - acc: 0.9747 - val_loss: 0.0749 - val_acc: 0.9793\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0863 - acc: 0.9741 - val_loss: 0.0719 - val_acc: 0.9802\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0846 - acc: 0.9749 - val_loss: 0.0752 - val_acc: 0.9796\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0805 - acc: 0.9760 - val_loss: 0.0725 - val_acc: 0.9808\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0813 - acc: 0.9753 - val_loss: 0.0734 - val_acc: 0.9807\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0815 - acc: 0.9761 - val_loss: 0.0745 - val_acc: 0.9802\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0795 - acc: 0.9764 - val_loss: 0.0732 - val_acc: 0.9806\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0784 - acc: 0.9772 - val_loss: 0.0760 - val_acc: 0.9800\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0771 - acc: 0.9774 - val_loss: 0.0744 - val_acc: 0.9806\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0769 - acc: 0.9776 - val_loss: 0.0779 - val_acc: 0.9792\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0770 - acc: 0.9766 - val_loss: 0.0801 - val_acc: 0.9796\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in]+n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim, kernel_initializer=TruncatedNormal(stddev=0.01)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "model.add(Dense(n_out, kernel_initializer=TruncatedNormal(stddev=0.01)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
